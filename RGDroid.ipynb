{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "from scipy.sparse import coo_matrix\n",
    "import networkx as nx\n",
    "import torch\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑总内存：7.9150 GB\n",
      "当前使用的总内存占比： 89.8\n",
      "cpu个数： 4\n",
      "当前进程的内存使用：0.3290 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "info = psutil.virtual_memory()\n",
    "print(u'电脑总内存：%.4f GB' % (info.total / 1024 / 1024 / 1024) )\n",
    "print(u'当前使用的总内存占比：',info.percent)\n",
    "print(u'cpu个数：',psutil.cpu_count())\n",
    "print(u'当前进程的内存使用：%.4f GB' % (psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.sparse._coo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-54e9f3eecdbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"test_data.pkl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mall_train_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy.sparse._coo'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# train = pd.read_csv(\"../copy-malscan/split_train.csv\")\n",
    "train_path = \"test_data.pkl\"\n",
    "df = open(train_path, \"rb\")\n",
    "all_train_dict = pickle.load(df)\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../APIGraph-master/src/res/method_entity_embedding_TransE.pkl', 'rb') as f:\n",
    "    entity_embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_all = all_train_dict[\"noedge_weight\"]\n",
    "all_sha = all_train_dict[\"all_sha\"]\n",
    "node_all = all_train_dict[\"node_all\"]\n",
    "all_sen_idx = all_train_dict[\"all_sen_idx\"]\n",
    "all_label = all_train_dict[\"all_label\"]\n",
    "cons_fea = all_train_dict[\"cons_fea\"]\n",
    "cons = all_train_dict[\"cons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from:  ../flow_graph_exp/GCN_OUR_CON_NEW.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GCN_OUR(\n",
       "  (conv1): GraphConv(in=10, out=64, normalization=none, activation=None)\n",
       "  (conv2): GraphConv(in=64, out=64, normalization=none, activation=None)\n",
       "  (conv3): GraphConv(in=64, out=64, normalization=none, activation=None)\n",
       "  (classify): Linear(in_features=192, out_features=1, bias=True)\n",
       "  (attn_fc): Linear(in_features=128, out_features=1, bias=False)\n",
       "  (pooling): MaxPooling()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home2/ykli/workspace/flow_graph_exp/\")\n",
    "from Models import GCN_OUR\n",
    "model_dir = \"../flow_graph_exp/GCN_OUR_CON_NEW.pth\"\n",
    "model = GCN_OUR(10,64,1)\n",
    "model.load_state_dict(torch.load(model_dir)[\"model\"])\n",
    "print('loaded model from: ', model_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "commufile = \"../flow_graph_exp/all_commu_new.pkl\"\n",
    "with open(commufile, 'rb') as f:\n",
    "    commu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1960895/1570822989.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  commu = np.array(commu)[train[train.test==True].index.values]\n"
     ]
    }
   ],
   "source": [
    "commu = np.array(commu)[train[train.test==True].index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1960895/136646228.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_edge_weight = np.array(all_edge_weight)\n"
     ]
    }
   ],
   "source": [
    "test_df = train[train.test!=False]\n",
    "with open(\"../flow_graph_exp/edge_weight_new_con.pkl\", 'rb') as f:\n",
    "    final_edge = pickle.load(f)\n",
    "all_edge_weight = final_edge[\"all_edge_weight\"]\n",
    "all_sens_edge_weight = final_edge[\"all_sens_edge_weight\"]\n",
    "all_edge_weight = np.array(all_edge_weight)\n",
    "all_ww = all_edge_weight[test_df.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4004/4004 [00:34<00:00, 115.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct_indices = []\n",
    "# all_graphs = X\n",
    "# all_labels = Y\n",
    "all_g = []\n",
    "all_pred = []\n",
    "\n",
    "all_C = []\n",
    "all_S = []\n",
    "all_N = []\n",
    "all_sens = []\n",
    "all_L = []\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for i in tqdm(range(len(all_label))):\n",
    "    sample = adj_all[i].T\n",
    "    g = dgl.from_scipy(sample)\n",
    "    num_nodes = sample.shape[0]\n",
    "    nodes = node_all[i]\n",
    "    cc = cons_fea[i]\n",
    "    c = cons[i]\n",
    "    fea = np.zeros((num_nodes, 10))\n",
    "    for ks in cc:\n",
    "        if ks>=num_nodes:\n",
    "            continue\n",
    "        temp_node = nodes[ks][1:-1].split(\":\")\n",
    "        ids = temp_node[0] + \".\" + temp_node[1].strip().split(\" \")[-1].split(\"(\")[0]\n",
    "        if ids in entity_embedding:\n",
    "            fea[ks] = entity_embedding[ids]\n",
    "    \n",
    "    edge_weight  = all_ww[i]\n",
    "    g.edata['h'] = torch.tensor(edge_weight).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # for baseline\n",
    "        # fea = g.in_degrees().view(-1, 1).float()\n",
    "        # g.ndata['h'] = fea\n",
    "        g.ndata['h'] = torch.tensor(fea).float()\n",
    "        # g = g.to(\"cuda\")\n",
    "        all_g.append(copy.deepcopy(g))\n",
    "        preds = model.cpu()(g).detach()\n",
    "        \n",
    "    all_pred.append(preds)\n",
    "    all_L.append(all_label[i])\n",
    "    all_C.append(c)\n",
    "    all_S.append(cc)\n",
    "    all_N.append(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = torch.cat(all_pred).reshape(-1).sigmoid()\n",
    "isright = np.array((all_pred>0.5).cpu())==np.array(all_L)\n",
    "selected_indices = np.where(isright==True)[0]\n",
    "correct_indices = selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (all_pred.reshape(-1)>0.5).cpu().detach().numpy().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score\n",
    "def get_metric(test_Y, y_pred):\n",
    "    f1 = f1_score(y_true=test_Y, y_pred=y_pred)\n",
    "    precision = precision_score(y_true=test_Y, y_pred=y_pred)\n",
    "    recall = recall_score(y_true=test_Y, y_pred=y_pred)\n",
    "    accuracy = accuracy_score(y_true=test_Y, y_pred=y_pred)\n",
    "    TP = np.sum(np.multiply(test_Y, y_pred))\n",
    "    FP = np.sum(np.logical_and(np.equal(test_Y, 0), np.equal(y_pred, 1)))\n",
    "    FN = np.sum(np.logical_and(np.equal(test_Y, 1), np.equal(y_pred, 0)))\n",
    "    TN = np.sum(np.logical_and(np.equal(test_Y, 0), np.equal(y_pred, 0)))\n",
    "\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = FN / (TP + FN)\n",
    "    return f1, precision, recall, accuracy, TPR, FPR, TNR, FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9592508225765629,\n",
       " 0.9722934838378656,\n",
       " 0.9465534465534465,\n",
       " 0.9597902097902098,\n",
       " 0.9465534465534465,\n",
       " 0.026973026973026972,\n",
       " 0.973026973026973,\n",
       " 0.05344655344655345)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric(all_L, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
